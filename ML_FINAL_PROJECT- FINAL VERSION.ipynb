{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd91f0d6",
   "metadata": {},
   "source": [
    "# SPAM FILTER\n",
    "\n",
    "**In this project we are implementing a spam filter to detect spam emails using NLP techniques there are 3 different approaches we have tried in this project** \n",
    "\n",
    "* 1. Using a pretrained model(FastText) model and training it on our email dataset to generate document vectors and then use the feature embeddings of the doucument vectors of every email along with other covariates to predict wheather an email is spam or not \n",
    "\n",
    "* 2. Using a Word2Vec model trained on the email dataset to generate document vectors and then use the feature embeddings of the doucument vectors generated for every email to predict wheather it is spam or not *\n",
    "\n",
    "* 3. Using TFIDF on the email dataset for feature extraction and then use the feature embeddings of the doucument vectors generated for every email to predict wheather it is spam or not \n",
    "\n",
    "The choices made in building each model is discussed along with the code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217a394c",
   "metadata": {},
   "source": [
    "**OVERVIEW OF THE APPROACH**\n",
    "\n",
    "The first method we will be exploring and the best model out all the 3 models explored is the one built using a pretrained fast text model and training it on our email data set to capture the context in the email data set and then using the feature embeddings of the new_model (name of the fast text pretrained model that has been trained on our email dataset) to generate word vectors. \n",
    "Next we build document vectors by taking the mean of the word vectors in each email and then add 2 covirates to the document vectors (which have been discussed in the relevant sections below) to predict the label of the email - spam / not "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22a010a",
   "metadata": {},
   "source": [
    "**SECTION 1.1 - Load the Dataset and do EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2bf077bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jyomohan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "nltk.data.path.append(os.path.expanduser('/Users/jyomohan/Downloads'))\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2844b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the CSV file\n",
    "#path = '/Users/jyomohan/Downloads/spam_ham_dataset.csv'\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "68da4ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>605</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2349</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3624</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4685</td>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2030</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 label                                               text  \\\n",
       "0         605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
       "1        2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
       "2        3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
       "3        4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
       "4        2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
       "\n",
       "   label_num  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          1  \n",
       "4          0  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0c78a1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  label_num\n",
       "0   ham  Subject: enron methanol ; meter # : 988291\\r\\n...          0\n",
       "1   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...          0\n",
       "2   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...          0\n",
       "3  spam  Subject: photoshop , windows , office . cheap ...          1\n",
       "4   ham  Subject: re : indian springs\\r\\nthis deal is t...          0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop the unamed column\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "02f3f63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Subject: enron methanol ; meter # : 988291\\r\\n...\n",
       "1   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...\n",
       "2   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...\n",
       "3  spam  Subject: photoshop , windows , office . cheap ...\n",
       "4   ham  Subject: re : indian springs\\r\\nthis deal is t..."
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop any of the column label_num\n",
    "df = df.drop(['label_num'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f3b5b642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 5171 emails\n"
     ]
    }
   ],
   "source": [
    "#check the total number of data rows\n",
    "print('found %s emails'% len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "98f18761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     3672\n",
       "spam    1499\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the amount of each label (spam and ham)\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "32d9648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the 'ham' label to 'non-spam'\n",
    "df['label'] = df['label'].replace(['ham'],'non-spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "74b465d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the distribution of the amount of each label onto the bar chart\n",
    "df_label = sns.countplot(df['label'])\n",
    "df_label.set_xticklabels(df['label'].unique())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8adcc10",
   "metadata": {},
   "source": [
    "In some cases, one class may be rare compared to the other class, and the goal of the model is to accurately predict the rare event. In this scenario, balancing the classes may lead to a model that is biased towards the majority class, resulting in poor performance on the rare class.\n",
    "\n",
    "We can see that there is an imbalanced class between the spam emails and the non spam emails, but we have chosen to not balance the classes because in most real life data, spam emails would be lesser than the legitimate emails one receives. Percentage of the spam class is clase to 40% which not very less."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84390c6",
   "metadata": {},
   "source": [
    "**SECTION 1.2 - Clean the dataset**\n",
    "\n",
    "for ceanining the data we remove punctuation, convert uppercase to lower case, remove multiple spaces, remove special characters, remove punction \n",
    "\n",
    "extracting covariates - there are 2 covriates that we are using apart from the document vector to predict spam emails  in the cleaning function:\n",
    "* COUNT OF ILLEGITIMATE EMAIL LINKS : we are also extraxting the links (if they exist) in each email and checking it against a domains of legitimate links. We increament the count variable if the link is not present in the list of legitimate links \n",
    "* COUNT OF UPPER CASE: we extract the number of upper case letters in each mail.\n",
    "\n",
    "The intuition behind using these 2 covariates apart from the word embeddings of the email is that - spam emails are highly likely to contain links of illegitimate websites / phising links and spam emails usually also contain a lot of Upper case letters. Thus we have used these 2 covariates in our analysis as they might be good predictors for spam emails "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4f05bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Cleaning\n",
    "import string\n",
    "\n",
    "punct = []\n",
    "for char in string.punctuation:\n",
    "    punct.append(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f22d71e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOWNLOAD FILE WITH LEGITIMATE LINKS\n",
    "import pandas as pd\n",
    "legitamate_domain_list=[]\n",
    "domains_df  = pd.read_excel('/Users/jyomohan/Downloads/list_of_domain.xlsx')\n",
    "for domain in domains_df['Domain Name']:\n",
    "    legitamate_domain_list.append(domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e1e4cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(txt):\n",
    "    \n",
    "    # count number of uppercase letters\n",
    "    count_uppercase = sum(1 for c in txt if c.isupper())\n",
    "    \n",
    "    # remove links\n",
    "    new = []\n",
    "    count = 0\n",
    "    links = re.findall(r'((https).+|(http).+|(www).+)', txt)\n",
    "    for a in links:\n",
    "        my_website = re.sub(r\".+www .(.+?\\. com).+\", r\"\\1\", str(links))\n",
    "        new_obj = re.sub(r\" \", \"\", my_website)\n",
    "        new.append(re.sub(r\" \", \"\", my_website))\n",
    "        if new_obj not in legitamate_domain_list:\n",
    "            count += 1\n",
    "        #if new_obj not in domains_df['Domain Name']:\n",
    "        #if any(domains_df['Domain Name'].apply(lambda x: x not in a)):\n",
    "            #count +=1\n",
    "        else:\n",
    "            count = 0\n",
    "    text = txt.replace(\"http ://\", \" \").replace(\"https ://\", \" \").replace(\"www .\", \" \")\n",
    "    \n",
    "    # case folding\n",
    "    text = txt.lower()\n",
    "    \n",
    "    # remove multiple space, tabs, and newlines\n",
    "    text = re.sub('\\s+',' ',text)\n",
    "    \n",
    "    \n",
    "    # remove special characters\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = ''.join([word for word in text if word not in punct])\n",
    "    \n",
    "    #remove single character\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "    \n",
    "    #remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    \n",
    "    #remove multiple spaces (again)\n",
    "    text = re.sub('\\s+',' ',text)\n",
    "    \n",
    "    #print(new)\n",
    "    return text, count, count_uppercase\n",
    "    #print(text)\n",
    "    #print(new_obj)\n",
    "    #print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbb4b5d",
   "metadata": {},
   "source": [
    "**Checking - THE CLEANING FUNCTION** \n",
    "\n",
    "Next we are checking the cleaning function if it counts the number of illegitimate links correctly, we access the 951st email in the df object. it contains the link of \"amazon.com\" we can see that the count for the illegitimate links is returned as zero. \n",
    "\n",
    "Next we are checking the same for the 68th email that contains an illegitimate link, we can see that the count is returned correctly 1 for the website \"www.fintod.com' which is not present in the list of legitimate websites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a70afa49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('subject your amazon com order greetings from amazon com we thought you like to know that we shipped your items today and that this completes your order thanks for shopping at amazon com and we hope to see you again soon you can track the status of this order and all your orders online by visiting your account page at http www amazon com your account there you can track order and shipment status review estimated delivery dates cancel unshipped items return items and do much more the following items were included in this shipment qty item price shipped subtotal tonka mighty front loader item subtotal shipping handling total this shipment was sent to daren farmer meadowtree spring tx via ups ground business days for your reference the number you can use to track your package is lza wl you can refer to our web site help page or http www amazon com tracking to retrieve current tracking information please note that tracking information may not be available immediately if you ve explored the links on your account page but still need to get in touch with us about your order mail us at orders amazon com on your next visit to our web site come see what new for you by clicking the link on the right hand side of our home page or by visiting this url we ve selected an assortment of new releases recommendations and informative articles that we think would appeal to you thank you for shopping at amazon com amazon com earth biggest selection orders amazon com http www amazon com ',\n",
       " 0,\n",
       " 1)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning(df['text'][951])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0ea2a968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('subject re husband soup would be as you know election time is not the best thing for the economy economy is in very unstable condition as you can see gas prices are going up along with the rtgvage rat once the te goes up you will not have chance to av money again for very long time it is your last chance get inanced at point http www fintod com despoil compote amende the me orbital irruption gfawn ax henrietta the in boatswain out whither the accompanist lint macintosh',\n",
       " 1,\n",
       " 1)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning(df['text'][68])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "77c143f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>count_of_spam_links</th>\n",
       "      <th>count_uppercase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>subject enron methanol meter this is follow up...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>subject hpl nom for january see attached file ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>subject neon retreat ho ho ho we re around to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "      <td>subject photoshop windows office cheap main tr...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "      <td>subject re indian springs this deal is to book...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  \\\n",
       "0  non-spam  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
       "1  non-spam  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
       "2  non-spam  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
       "3      spam  Subject: photoshop , windows , office . cheap ...   \n",
       "4  non-spam  Subject: re : indian springs\\r\\nthis deal is t...   \n",
       "\n",
       "                                        text_cleaned  count_of_spam_links  \\\n",
       "0  subject enron methanol meter this is follow up...                    0   \n",
       "1  subject hpl nom for january see attached file ...                    0   \n",
       "2  subject neon retreat ho ho ho we re around to ...                    1   \n",
       "3  subject photoshop windows office cheap main tr...                    0   \n",
       "4  subject re indian springs this deal is to book...                    0   \n",
       "\n",
       "   count_uppercase  \n",
       "0                1  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply cleaning function to every text and store the cleaned data, count of illegitimate links and count of upper case in a seperate variable\n",
    "df[['text_cleaned', 'count_of_spam_links', 'count_uppercase']] = df['text'].apply(lambda x: pd.Series(cleaning(x)))\n",
    "#df = df[['text', 'text_cleaned', 'label', 'count', 'count_of_spam_links']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "47184ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5171, 4)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop the old unclean text column and rename the cleaned text to \"text\"\n",
    "df = df.drop(['text'], axis=1)\n",
    "df = df.rename(columns = {'text_cleaned' : 'text'})\n",
    "df.columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "254faa5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4638, 4)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop NA values in the DF\n",
    "df_clean = df.dropna().drop_duplicates()\n",
    "df_clean.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f42b7ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       subject enron methanol meter this is follow up...\n",
       "1       subject hpl nom for january see attached file ...\n",
       "2       subject neon retreat ho ho ho we re around to ...\n",
       "3       subject photoshop windows office cheap main tr...\n",
       "4       subject re indian springs this deal is to book...\n",
       "                              ...                        \n",
       "5164    subject slutty milf wants to meet you take tha...\n",
       "5166    subject put the on the ft the transport volume...\n",
       "5167    subject and following noms hpl can take the ex...\n",
       "5169    subject industrial worksheets for august activ...\n",
       "5170    subject important online banking alert dear va...\n",
       "Name: text, Length: 4638, dtype: object"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffca3836",
   "metadata": {},
   "source": [
    "**SECTION 1.3 - IMPLEMENTING A PRETRAINED FAST TEXT MODEL**\n",
    "\n",
    "We are using the fast text model that has been modeled on a million words in wiki, then we check the model by looking at the nearest neighbors for the word \"kind\" - we get relevant words linked to the word kind, thus the pretrained model can effectvely capture meaning\n",
    "and we can see that the word embedding's using a fast text model is 300 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "eba2aa20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "model_en = fasttext.load_model('/Users/jyomohan/Downloads/cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "74807655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_labels',\n",
       " '_words',\n",
       " 'f',\n",
       " 'get_analogies',\n",
       " 'get_dimension',\n",
       " 'get_input_matrix',\n",
       " 'get_input_vector',\n",
       " 'get_label_id',\n",
       " 'get_labels',\n",
       " 'get_line',\n",
       " 'get_meter',\n",
       " 'get_nearest_neighbors',\n",
       " 'get_output_matrix',\n",
       " 'get_sentence_vector',\n",
       " 'get_subword_id',\n",
       " 'get_subwords',\n",
       " 'get_word_id',\n",
       " 'get_word_vector',\n",
       " 'get_words',\n",
       " 'is_quantized',\n",
       " 'labels',\n",
       " 'predict',\n",
       " 'quantize',\n",
       " 'save_model',\n",
       " 'set_args',\n",
       " 'set_matrices',\n",
       " 'test',\n",
       " 'test_label',\n",
       " 'words']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "da63d919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8196556568145752, 'sort'),\n",
       " (0.6016196608543396, 'really'),\n",
       " (0.5989708304405212, 'kindof'),\n",
       " (0.5845443606376648, 'Kind'),\n",
       " (0.5845313668251038, 'type'),\n",
       " (0.5710219740867615, 'thing'),\n",
       " (0.5651149749755859, 'kinds'),\n",
       " (0.56373530626297, 'kinda'),\n",
       " (0.5608729720115662, 'weird'),\n",
       " (0.5422527194023132, 'kind-of')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_en.get_nearest_neighbors(\"kind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "42923e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_en.get_word_vector(\"kind\").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fecba2",
   "metadata": {},
   "source": [
    "**SECTION 1.4 - TRAINING THE PRETRAINED FAST TEXT MODEL ON OUR EMAIL DATA SET**\n",
    "\n",
    "Training a the fast text model on the cleaned email text saved in the CSV, check the dimensions of the output vector and then see if the nearest neighbors detected by the new model can capture context. We can see that it can effectively capture context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d3078d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the cleaned email text alone in aseperate CSV file\n",
    "df.to_csv(\"clean_email_data.txt\", columns=[\"text\"], header = None, index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6f036f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  9027\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   45197 lr:  0.000000 avg.loss:  1.948293 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training a the fast text model on the cleaned email text saved in the CSV \n",
    "new_model = fasttext.train_unsupervised(\"clean_email_data.txt\", dim=300)\n",
    "new_model.get_word_vector(\"bank\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cf4fc8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7534722089767456, 'kindly'),\n",
       " (0.7362703680992126, 'mind'),\n",
       " (0.6885408163070679, 'kin'),\n",
       " (0.6720021963119507, 'construed'),\n",
       " (0.6583312153816223, 'shail'),\n",
       " (0.6187035441398621, 'sha'),\n",
       " (0.6140446662902832, 'constitute'),\n",
       " (0.6032567620277405, 'shall'),\n",
       " (0.5821604132652283, 'soiicitation'),\n",
       " (0.5802438855171204, 'constitutes')]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the nearest neighbors of the word kind, to see if it can capture the meaning\n",
    "new_model.get_nearest_neighbors(\"kind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a6e37bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check dimensions of the word embeddings from the new model\n",
    "new_model.get_word_vector(\"bank\").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d69d643",
   "metadata": {},
   "source": [
    "**SECTION 1.5 - GENERATE DOCUMENT VECTORS **\n",
    "\n",
    "Build a document vector by taking the mean of all the word vectors in each email and use that to predict the spam emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1f95b657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    non-spam       0.98      0.99      0.99       742\n",
      "        spam       0.97      0.96      0.96       293\n",
      "\n",
      "    accuracy                           0.98      1035\n",
      "   macro avg       0.98      0.97      0.97      1035\n",
      "weighted avg       0.98      0.98      0.98      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Open the file containing the clean email data\n",
    "with open(\"clean_email_data.txt\", \"r\") as f:\n",
    "    email_data = f.readlines()\n",
    "\n",
    "# Define a function to convert a list of word embeddings into a document vector\n",
    "def get_doc_vector(word_embeddings):\n",
    "    doc_vector = np.mean(word_embeddings, axis=0)\n",
    "    return doc_vector\n",
    "\n",
    "# Initialize list to store document vectors\n",
    "doc_vectors = []\n",
    "\n",
    "# Loop through each email in the data\n",
    "for email in email_data:\n",
    "    # Split the email into words\n",
    "    words = email.strip().split()\n",
    "    \n",
    "    # Get the word embeddings for each word\n",
    "    word_embeddings = []\n",
    "    for word in words:\n",
    "        word_embeddings.append(new_model.get_word_vector(word))\n",
    "    \n",
    "    # Convert the word embeddings into a document vector\n",
    "    doc_vector = get_doc_vector(word_embeddings)\n",
    "    \n",
    "    \n",
    "    # Add the document vector to the list\n",
    "    doc_vectors.append(doc_vector)\n",
    "\n",
    "# Convert the list of document vectors to a NumPy array\n",
    "doc_vectors = np.array(doc_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388e80ff",
   "metadata": {},
   "source": [
    "In this logistic regression model, the covariates used to predict whether an email is spam or non-spam are document vectors and the number of illegitimate links and the number of uppercase letters in each email.\n",
    "\n",
    "Document vectors are numerical representations of the content of the emails, which were obtained using a pre-trained FastText model that was further fine-tuned on the email dataset. FastText is a popular natural language processing algorithm that is capable of generating high-quality vector representations of text. By fine-tuning the pre-trained FastText model on the email dataset, the model can learn to generate document vectors that are specifically tailored to the characteristics of email text, which can help to improve the accuracy of the logistic regression model in predicting spam and non-spam emails.\n",
    "\n",
    "The number of illegitimate links in each email is another covariate used by the model to predict spam. Spam emails often contain suspicious links or URLs that are used to direct users to phishing sites or other malicious content. Therefore, including this variable in the model may help to capture the presence of such links and improve the model's ability to differentiate between spam and non-spam emails.\n",
    "\n",
    "By using these covariates in the logistic regression model, the model can learn how to weigh the importance of different features in predicting spam emails, and provide a prediction for each email based on its specific features.\n",
    "\n",
    "\n",
    "**SECTION 1.6 - BUILD A LOGIT MODEL TO PREDICT SPAM EMAILS BASED ON THE DOC VECTORS GENERATED**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4101375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vec_df = pd.DataFrame(doc_vectors)\n",
    "doc_vec_df['count_of_spam_links'] = df['count_of_spam_links']\n",
    "doc_vec_df['count_uppercase'] = df['count_uppercase']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9e391a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5171, 302)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1c98c1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    non-spam       0.98      0.99      0.99       742\n",
      "        spam       0.97      0.96      0.96       293\n",
      "\n",
      "    accuracy                           0.98      1035\n",
      "   macro avg       0.97      0.97      0.97      1035\n",
      "weighted avg       0.98      0.98      0.98      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(doc_vec_df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Logistic Regression model\n",
    "y_pred_proba = lr.predict_proba(X_test)\n",
    "y_pred = []\n",
    "cutoff = 0.6\n",
    "for prob in y_pred_proba:\n",
    "    if prob[1] >= cutoff:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "\n",
    "# Decode labels\n",
    "y_test = le.inverse_transform(np.array(y_test))\n",
    "y_pred = le.inverse_transform(np.array(y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f549cedb",
   "metadata": {},
   "source": [
    "**INFERENCE BASED ON OUTPUT OF LOGIT MODEL**\n",
    "\n",
    "Based on the output of the logistic regression, the model achieved an overall accuracy of 0.97 in predicting whether an email is spam or non-spam. The precision for non-spam emails is 0.97, meaning that 97% of the emails classified as non-spam were actually non-spam. The recall for non-spam emails is 0.99, indicating that the model correctly identified 99% of the non-spam emails.\n",
    "\n",
    "For spam emails, the precision is 0.98, implying that 98% of the emails classified as spam were indeed spam. The recall for spam emails is 0.92, meaning that the model correctly identified 92% of the spam emails.\n",
    "\n",
    "The F1-score, which is the harmonic mean of precision and recall, is 0.98 for non-spam emails and 0.95 for spam emails. These scores suggest that the model performed well in identifying non-spam emails but may have some difficulty in correctly identifying all spam emails.\n",
    "\n",
    "Overall, the model's performance appears to be satisfactory, with high precision and recall scores for both spam and non-spam emails, and a high F1-score for non-spam emails. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "01afb89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      y_pred_proba    y_test    y_pred\n",
      "0     2.189073e-07  non-spam  non-spam\n",
      "1     9.999905e-01      spam      spam\n",
      "2     4.653304e-06  non-spam  non-spam\n",
      "3     1.425434e-03  non-spam  non-spam\n",
      "4     9.000021e-04  non-spam  non-spam\n",
      "...            ...       ...       ...\n",
      "1030  9.768916e-01      spam      spam\n",
      "1031  3.484369e-01  non-spam  non-spam\n",
      "1032  9.818294e-01      spam      spam\n",
      "1033  2.386245e-01  non-spam  non-spam\n",
      "1034  1.109170e-06  non-spam  non-spam\n",
      "\n",
      "[1035 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create dataframe with y_pred_proba, y_test, and y_pred columns\n",
    "result_df = pd.DataFrame({'y_pred_proba': y_pred_proba[:, 1], 'y_test': y_test, 'y_pred': y_pred})\n",
    "\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff8dcc0",
   "metadata": {},
   "source": [
    "**SECTION 2 - OTHER APPROACHES WE EXPLORED**\n",
    "\n",
    "The other approaches we explored involved \n",
    "1. employing TFIDF For feature extraction \n",
    "2. Using Word2Vec for feature embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465e14be",
   "metadata": {},
   "source": [
    "**SECTION 2.1 - LOAD THE DATA AND DO EDA**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "232aae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/jyomohan/Downloads/spam_ham_dataset.csv'\n",
    "df_w2v = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf8f47b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>605</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2349</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3624</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4685</td>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2030</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 label                                               text  \\\n",
       "0         605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
       "1        2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
       "2        3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
       "3        4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
       "4        2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
       "\n",
       "   label_num  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          1  \n",
       "4          0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_w2v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e95a246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  label_num\n",
       "0   ham  Subject: enron methanol ; meter # : 988291\\r\\n...          0\n",
       "1   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...          0\n",
       "2   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...          0\n",
       "3  spam  Subject: photoshop , windows , office . cheap ...          1\n",
       "4   ham  Subject: re : indian springs\\r\\nthis deal is t...          0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_w2v = df_w2v.drop(['Unnamed: 0'], axis=1)\n",
    "df_w2v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bae2c13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Subject: enron methanol ; meter # : 988291\\r\\n...\n",
       "1   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...\n",
       "2   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...\n",
       "3  spam  Subject: photoshop , windows , office . cheap ...\n",
       "4   ham  Subject: re : indian springs\\r\\nthis deal is t..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop any of the column labels\n",
    "df_w2v = df_w2v.drop(['label_num'], axis=1)\n",
    "df_w2v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "564c7978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 5171 emails\n"
     ]
    }
   ],
   "source": [
    "#check the total number of data rows\n",
    "print('found %s emails'% len(df_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3462817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     3672\n",
       "spam    1499\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the amount of each label (spam and ham)\n",
    "df_w2v['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26157e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w2v['label'] = df_w2v['label'].replace(['ham'],'non-spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3404521f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the distribution of the amount of each label onto the bar chart\n",
    "df_w2v_label = sns.countplot(df_w2v['label'])\n",
    "df_w2v_label.set_xticklabels(df_w2v['label'].unique())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dbec7e",
   "metadata": {},
   "source": [
    "**SECTION 2.2 - CLEAN THE DATA**\n",
    "\n",
    "Same as what we did in the previous section \n",
    "But we also include lemmetization function as we are doing feature extraction using TFIDF. More about that in the note near the lemmetization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25feecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Cleaning\n",
    "import string\n",
    "\n",
    "punct = []\n",
    "for char in string.punctuation:\n",
    "    punct.append(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ce84b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(txt):\n",
    "    # case folding\n",
    "    text = txt.lower()\n",
    "    \n",
    "    # remove multiple space, tabs, and newlines\n",
    "    text = re.sub('\\s+',' ',text)\n",
    "    \n",
    "    # remove links\n",
    "    text = text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "    \n",
    "    # remove special characters\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = ''.join([word for word in text if word not in punct])\n",
    "    \n",
    "    #remove single character\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "    \n",
    "    #remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    \n",
    "    #remove multiple spaces (again)\n",
    "    text = re.sub('\\s+',' ',text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4f7b220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>subject enron methanol meter this is follow up...</td>\n",
       "      <td>non-spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>subject hpl nom for january see attached file ...</td>\n",
       "      <td>non-spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>subject neon retreat ho ho ho we re around to ...</td>\n",
       "      <td>non-spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "      <td>subject photoshop windows office cheap main tr...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "      <td>subject re indian springs this deal is to book...</td>\n",
       "      <td>non-spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
       "1  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
       "2  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
       "3  Subject: photoshop , windows , office . cheap ...   \n",
       "4  Subject: re : indian springs\\r\\nthis deal is t...   \n",
       "\n",
       "                                        text_cleaned     label  \n",
       "0  subject enron methanol meter this is follow up...  non-spam  \n",
       "1  subject hpl nom for january see attached file ...  non-spam  \n",
       "2  subject neon retreat ho ho ho we re around to ...  non-spam  \n",
       "3  subject photoshop windows office cheap main tr...      spam  \n",
       "4  subject re indian springs this deal is to book...  non-spam  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply cleaning function to every text\n",
    "df_w2v['text_cleaned'] = df_w2v['text'].apply(lambda x: cleaning(x))\n",
    "df_w2v = df_w2v[['text', 'text_cleaned', 'label']]\n",
    "df_w2v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1555a85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop word removal\n",
    "stop = stopwords.words('english')\n",
    "df_w2v['text_cleaned'] = df_w2v['text_cleaned'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e906e74",
   "metadata": {},
   "source": [
    "**Note :** \n",
    "1. Lemmetization and its relevance in feature extraction\n",
    "    By lemmatizing words before feature extraction, you can ensure that different forms of the same word are treated as the same feature. In summary, lemmatization can help to reduce the number of unique words in a dataset and ensure that different forms of the same word are treated as the same feature, leading to more accurate and effective feature extraction in natural language processing tasks.\n",
    "    \n",
    "2. It is also important that we did not lemmetize in the previous section where we built document vectors using the fast text pretrained model. As we are matching the format of email data to the format of the data that the pretrained model was trained on. (So we do not use lemmetization in the previous section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3300264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Lemmetization\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# mapping the POS tags\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)   \n",
    "\n",
    "def do_lemma(string):\n",
    "    lemmatized = ' '.join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in nltk.word_tokenize(string)])\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4e87df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "nltk.data.path.append('/Users/jyomohan/Downloads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cc7099b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw to /Users/jyomohan/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk\n",
    "nltk.download('omw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3e76828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/jyomohan/nltk_data',\n",
       " '/Users/jyomohan/opt/anaconda3/nltk_data',\n",
       " '/Users/jyomohan/opt/anaconda3/share/nltk_data',\n",
       " '/Users/jyomohan/opt/anaconda3/lib/nltk_data',\n",
       " '/usr/share/nltk_data',\n",
       " '/usr/local/share/nltk_data',\n",
       " '/usr/lib/nltk_data',\n",
       " '/usr/local/lib/nltk_data',\n",
       " '/Users/jyomohan/Downloads',\n",
       " '/Users/jyomohan/Downloads']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk\n",
    "nltk.data.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61f5cad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /Users/jyomohan/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94f7c3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw to /Users/jyomohan/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The strip bat be hang on their foot for best\n"
     ]
    }
   ],
   "source": [
    "#Check lemmetization \n",
    "\n",
    "import nltk\n",
    "nltk.download('omw')\n",
    "\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "lemmatized = ' '.join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in nltk.word_tokenize(sentence)])\n",
    "print(do_lemma(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f288f491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5171, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop the old uncleaned text column and replace with the clean text column\n",
    "df_w2v = df_w2v.drop(['text'], axis=1)\n",
    "df_w2v = df_w2v.rename(columns = {'text_cleaned' : 'text'})\n",
    "df_w2v.columns\n",
    "df_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12a4a481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4630, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop the NA values \n",
    "df_w2v_clean = df_w2v.dropna().drop_duplicates()\n",
    "df_w2v_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b91c606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       subject enron methanol meter follow note gave ...\n",
       "1       subject hpl nom january see attached file hpln...\n",
       "2       subject neon retreat ho ho ho around wonderful...\n",
       "3       subject photoshop windows office cheap main tr...\n",
       "4       subject indian springs deal book teco pvr reve...\n",
       "                              ...                        \n",
       "5164       subject slutty milf wants meet take ilaa liqaa\n",
       "5166    subject put ft transport volumes decreased con...\n",
       "5167    subject following noms hpl take extra mmcf wee...\n",
       "5169    subject industrial worksheets august activity ...\n",
       "5170    subject important online banking alert dear va...\n",
       "Name: text, Length: 4630, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the cleaned text column\n",
    "df_w2v_clean['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e9b0c0",
   "metadata": {},
   "source": [
    "**SECTION 2.3 - USE A TFIDF FOR FEATURE EXTRACTION**\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a feature extraction technique used in natural language processing to identify important words or phrases in a document. It works by assigning a weight to each term in a document based on how frequently it appears in the document and how rare it is across all documents in a corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f382ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF \n",
    "tfidf = TfidfVectorizer()\n",
    "X_w2v = tfidf.fit_transform(df_w2v['text'])\n",
    "y_w2v = df_w2v['label']\n",
    "#print(X_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9e392e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN TEST SPLIT\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(X_w2v, y_w2v, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33586fd",
   "metadata": {},
   "source": [
    "**SECTION 2.4 - USE A NAIVE BAYES FOR SPAM PREDICTION**\n",
    "\n",
    "Naive Bayes is a classification algorithm commonly used in natural language processing tasks such as text classification and spam detection. When combined with TF-IDF (Term Frequency-Inverse Document Frequency), it can be a powerful tool for predicting whether an email is spam or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2ef86601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Naive bayes\n",
    "clf_nb = MultinomialNB()\n",
    "clf_nb.fit(X_train_w2v, y_train_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3dcc21ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict using Naive Bayes\n",
    "pred_nb = clf_nb.predict(X_test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "196242a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95       742\n",
      "           1       1.00      0.73      0.84       293\n",
      "\n",
      "    accuracy                           0.92      1035\n",
      "   macro avg       0.95      0.86      0.90      1035\n",
      "weighted avg       0.93      0.92      0.92      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_w2v,pred_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceaa361",
   "metadata": {},
   "source": [
    "**SECTION 2.4 - USE A LOGIT MODEL FOR SPAM PREDICTION**\n",
    "\n",
    "Next we use Logistic regression for spam detection when combined with feature extraction using TF-IDF (Term Frequency-Inverse Document Frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9558881b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, penalty='l1', solver='liblinear')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classification LOGIT MODEL\n",
    "clf_lr=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l1')\n",
    "clf_lr.fit(X_train_w2v, y_train_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0fc59ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr = clf_lr.predict(X_test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "094ee4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    non-spam       0.99      0.96      0.98       742\n",
      "        spam       0.92      0.99      0.95       293\n",
      "\n",
      "    accuracy                           0.97      1035\n",
      "   macro avg       0.96      0.98      0.97      1035\n",
      "weighted avg       0.97      0.97      0.97      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_w2v,pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "626d6a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=4)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Forest \n",
    "clf_rf = RandomForestClassifier(n_estimators=4, criterion=\"gini\")\n",
    "clf_rf.fit(X_train_w2v, y_train_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a2e19115",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rf = clf_rf.predict(X_test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7200d03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    non-spam       0.93      0.99      0.96       742\n",
      "        spam       0.96      0.83      0.89       293\n",
      "\n",
      "    accuracy                           0.94      1035\n",
      "   macro avg       0.95      0.91      0.93      1035\n",
      "weighted avg       0.94      0.94      0.94      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_w2v,pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a977b05",
   "metadata": {},
   "source": [
    "When comparing the Naive Bayes, logistic regression, and random forest models for spam detection using TF-IDF, we can see that all three models perform relatively well.\n",
    "\n",
    "The logistic regression model achieves the highest accuracy of 97%, followed by the random forest model with an accuracy of 94%, and the Naive Bayes model with an accuracy of 92%. In terms of precision, the logistic regression model performs the best, achieving a precision of 92%, followed by the random forest model with a precision of 93%, and the Naive Bayes model with a precision of 100%. In terms of recall, the Naive Bayes model performs the best, achieving a recall of 73%, followed by the random forest model with a recall of 83%, and the logistic regression model with a recall of 99%. In terms of F1-score, the logistic regression model performs the best with a score of 95%, followed by the random forest model with a score of 89%, and the Naive Bayes model with a score of 84%.\n",
    "\n",
    "Overall, it seems that the logistic regression model is the best choice for spam detection using TF-IDF, as it achieves the highest accuracy and F1-score among the three models. However, the choice of model ultimately depends on the specific dataset and problem at hand, and it's always a good idea to try out multiple models and compare their performance before making a final decision.\n",
    "\n",
    "**SECTION 3 - USING WORD2VEC FOR PREDICTING SPAM EMAILS**\n",
    "\n",
    "**Purpose of using a word2vec over a TFIDF**\n",
    "\n",
    "Word2Vec is a neural network-based approach that captures the semantic relationships between words and is useful for applications such as sentiment analysis and text generation. In contrast, TF-IDF assigns a weight to each word based on its frequency in a document, and is useful for identifying important and specific words in a set of documents. While Word2Vec can be more powerful in capturing the meaning and context of words. In general, Word2Vec can be more powerful than TF-IDF in capturing the meaning and context of words in a document or corpus. This is because Word2Vec considers the relationships between words, whereas TF-IDF only considers the frequency of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10d9dbc",
   "metadata": {},
   "source": [
    "**SECTION 3.1 - TRAINING A WORD2VEC MODEL ON THE EMAIL DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "01c56207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentences = df_w2v['text']\n",
    "documents = [word_tokenize(sentence) for sentence in sentences]\n",
    "w2v_model = Word2Vec(\n",
    "        documents,\n",
    "        vector_size=150,\n",
    "        window=10,\n",
    "        min_count=2,\n",
    "        workers=10,\n",
    "        epochs=10,\n",
    "        sg = 1 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe5a191",
   "metadata": {},
   "source": [
    "**SECTION 3.2 - GENERATING DOCUMENT VECTORS**\n",
    "\n",
    "We generate the document vectors by taking the mean of each word inside each email "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5f9501b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# initialize an empty list to store the document vectors\n",
    "document_vectors = []\n",
    "\n",
    "# loop through every email in df['text']\n",
    "for email_text in df_w2v['text']:\n",
    "    # tokenize the email text into a list of words\n",
    "    words = word_tokenize(email_text)\n",
    "    \n",
    "    # initialize an empty list to store the word vectors for this document\n",
    "    word_vectors = []\n",
    "    \n",
    "    # loop through every word in the email text\n",
    "    for word in words:\n",
    "        # get the word vector from the Word2Vec model\n",
    "        try:\n",
    "            vector = w2v_model.wv[word]\n",
    "        except KeyError:\n",
    "            # if the word is not in the vocabulary, skip it\n",
    "            continue\n",
    "        \n",
    "        # add the word vector to the list of word vectors for this document\n",
    "        word_vectors.append(vector)\n",
    "    \n",
    "    # calculate the average word vector for this document\n",
    "    if len(word_vectors) > 0:\n",
    "        document_vector = sum(word_vectors) / len(word_vectors)\n",
    "    else:\n",
    "        # if the document has no words in the vocabulary, use a zero vector\n",
    "        document_vector = [0] * w2v_model.vector_size\n",
    "    \n",
    "    # add the document vector to the list of document vectors\n",
    "    document_vectors.append(document_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b26269b",
   "metadata": {},
   "source": [
    "**SECTION 3.3 - USE A LOGIT MODEL TO PREDICT THE SPAM EMAILS ON THE WORD2VEC DOC VECTORS**\n",
    "\n",
    " After representing emails as a vector of Word2Vec word embeddings, we can feed them into a logistic regression model for classification. The model learns to identify patterns and relationships between the word embeddings to accurately classify spam emails. Logistic regression is a simple yet powerful algorithm that can effectively model binary outcomes, making it a suitable choice for spam classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3b9e0f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    non-spam       0.99      0.99      0.99       742\n",
      "        spam       0.98      0.98      0.98       293\n",
      "\n",
      "    accuracy                           0.99      1035\n",
      "   macro avg       0.98      0.98      0.98      1035\n",
      "weighted avg       0.99      0.99      0.99      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_w2v = le.fit_transform(df_w2v['label'])\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(document_vectors, y_w2v, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_w2v, y_train_w2v)\n",
    "\n",
    "# Evaluate Logistic Regression model\n",
    "y_pred_proba_w2v = lr.predict_proba(X_test_w2v)\n",
    "y_pred_w2v = []\n",
    "cutoff = 0.6\n",
    "for prob in y_pred_proba_w2v:\n",
    "    if prob[1] >= cutoff:\n",
    "        y_pred_w2v.append(1)\n",
    "    else:\n",
    "        y_pred_w2v.append(0)\n",
    "\n",
    "# Decode labels\n",
    "y_test_w2v = le.inverse_transform(np.array(y_test_w2v))\n",
    "y_pred_w2v = le.inverse_transform(np.array(y_pred_w2v))\n",
    "\n",
    "print(classification_report(y_test_w2v, y_pred_w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fefc5fd",
   "metadata": {},
   "source": [
    "**SECTION 3.4 INFERENCE**\n",
    "\n",
    "Comparing the logistic regression model outputs after using Word2Vec and TFIDF for feature extraction to detect spam emails, it appears that the Word2Vec model has a slightly higher overall accuracy and f1-score, as well as better recall for identifying spam emails. This is likely because Word2Vec captures the semantic relationships between words and documents, which can provide better context for classification tasks like spam detection. However, both models have high precision, which indicates a low false positive rate, meaning the models are good at identifying non-spam emails. Overall, using either TFIDF or Word2Vec models with logistic regression can be an effective approach for detecting spam emails, with Word2Vec offering a slight advantage in accuracy and recall.\n",
    "\n",
    "**WHY IS WORD2VEC SUPERIOR TO TFIDF**\n",
    "\n",
    "Which makes sense as, Word2Vec is better than TF-IDF for feature embedding because it captures the semantic relationships between words, which is not possible with TF-IDF. While TF-IDF provides a measure of how important a word is in a document, it does not consider the context or meaning of the word. In contrast, Word2Vec represents words as vectors that encode their meanings based on the surrounding words in the text. This allows the model to capture the semantic relationships between words and represents them as geometric vectors in a high-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c56544",
   "metadata": {},
   "source": [
    "**Comparing Word2Vec and Fasttext Models when it comes to caturing meaning**\n",
    "\n",
    "Below we can see that the meaning of Bank is captured better by the pretrained model, as the most similar words are more relevant in that case than using just a word2Vec. \n",
    "\n",
    "FastText is a supervised learning algorithm for text classification that is built upon word embeddings (like Word2Vec), but with the additional capability of capturing subword information. Specifically, FastText uses character n-grams (sequences of consecutive characters) to represent words as vectors, in addition to the traditional word-level embeddings. This allows for the representation of words that may not appear in the training data set, as well as better handling of misspellings and out-of-vocabulary words.\n",
    "\n",
    "When we fine-tune a pre-trained FastText model on our email data set, we are essentially adapting the existing word embeddings to the specific language and vocabulary used in our emails. This is different from training a Word2Vec model from scratch on our data set, which may not be able to capture the nuances and context-specific meaning of the words used in the emails.\n",
    "\n",
    "By using a pre-trained FastText model as a starting point, we are able to leverage the large amounts of data used to train the original model and still achieve good performance on our specific task of spam detection. Fine-tuning the model on our email data set further improves its ability to capture meaning and context, leading to more accurate predictions compared to just using a Word2Vec model trained on our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a843ce27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('favourable', 0.7140632271766663),\n",
       " ('kenob', 0.696051299571991),\n",
       " ('cbn', 0.6760240197181702),\n",
       " ('syn', 0.6651967763900757),\n",
       " ('foreigner', 0.6628462672233582),\n",
       " ('tsb', 0.6625844240188599)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding most similar words to Bank using Word2Vec\n",
    "w2v_model.wv.most_similar('bank', topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4be08715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7481255531311035, 'banking'),\n",
       " (0.7420305609703064, 'citibank'),\n",
       " (0.7284749150276184, 'foreign'),\n",
       " (0.7118096947669983, 'foreigner'),\n",
       " (0.7113580703735352, 'banks'),\n",
       " (0.6866760849952698, 'citizensr'),\n",
       " (0.6829718351364136, 'citizen'),\n",
       " (0.656015932559967, 'bankrupt'),\n",
       " (0.6445817351341248, 'mr'),\n",
       " (0.6425755023956299, 'fund')]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding most similar words to Bank using pretrained fast text model\n",
    "new_model.get_nearest_neighbors(\"bank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92096caa",
   "metadata": {},
   "source": [
    "# CONCLUSION : \n",
    "\n",
    "In conclusion, our experiment on predicting spam emails has shown that by tuning a FastText model on the email dataset and then using a logistic regression model for prediction, we achieved superior results compared to other feature embedding techniques. \n",
    "\n",
    "Even though our Word2Vec model gave good results in predctions our pretrained model was superior in capturing the meaning of words, as it was pretrained on millions of words and hence captures the general context very well \n",
    "\n",
    "Specifically, our FastText model trained on the email dataset was able to capture important semantic information in the email text, which was further refined by the logistic regression model. Compared to TFIDF and Word2Vec, our approach demonstrated better accuracy, precision, recall, and F1-score in spam detection. While Word2Vec was slightly superior to TFIDF due to its ability to capture meaning, our approach using a FastText model for feature extraction and a logistic regression model for prediction was the most effective method for detecting spam emails in this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c02b5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
